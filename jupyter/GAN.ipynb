{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network for Augmentation of Laser-Based Laryngeal Imaging\n",
    "\n",
    "For the deep-learning-based algorithm to match features via a registration task, it is essential to apply intense data augmentation to the training data set. As displayed in the following figure the training data consists of images $m(x)$ that represent the spatial configuration of laser points projected onto the vocal fold surface.\n",
    "\n",
    "![alt text here](images/feature_matching_registration.png \"Logo Title Jupyter Notebook logo\")\n",
    "\n",
    "The foundation for the images $m(x)$ are the x-y-coordinates of each single laser point within the image, as $m(x)$ is generated by plotting the single laser points and then smoothing the image. To create intense augmentation we want to train a generative adversaraial network (GAN) to then generate images that are variations of the images of the training set and represent feasible configurations of laser points projected onto a vocal fold. The implementation of the VAE is inspired by Chollet [1].\n",
    "\n",
    "[1]F. Chollet, Deep Learning with Python, 1st ed. Shelter Island, New York: Manning Publications, 2017.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements\n",
    "The notebook was developed on Keras using the Tensorflow 2.2.0 backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Configuration\n",
    "Check for GPU and allow memory growth such that limitations for training are reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.get_visible_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters\n",
    "The grid dimensions have to be know, as well as the image dimensions for scaling. The depth of the input and output layer is 2 here, as we will have one channel representing x-coordinates and a second channel representing y-coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 18\n",
    "width = 18\n",
    "channels = 2\n",
    "\n",
    "image_width = 728\n",
    "image_height = 728"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "The dimension of the latent space can be adapted to optimize the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "The first part of a GAN is a generator network that takes random input vectors from the latent space and decodes the vector to generate a synthetic image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 10368)             93312     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 10368)             0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 9, 9, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 9, 9, 256)         819456    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 9, 9, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 18, 18, 256)       262400    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 18, 18, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 18, 18, 256)       1638656   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 18, 18, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 18, 18, 2)         25090     \n",
      "=================================================================\n",
      "Total params: 4,477,570\n",
      "Trainable params: 4,477,570\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# Leaky ReLU is preferred as it lowers the sparsity of gradients\n",
    "x = layers.Dense(128 * 9 * 9)(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape((9, 9, 128))(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Kernel size should be divisible by the stride size to prevent checkerboard artifacts\n",
    "x = layers.Conv2DTranspose(256, 2, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(256, 5, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Use tanh activation for improved training\n",
    "x = layers.Conv2D(channels, 7, activation='tanh', padding='same')(x)\n",
    "generator = keras.models.Model(generator_input, x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "The second part of the GAN is a discriminator network that takes an image as input and decides if the image comes from the training set or was synthetically created by the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_2 (InputLayer)         [(None, 18, 18, 2)]       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 16, 16, 128)       2432      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_5 (LeakyReLU)    (None, 16, 16, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 7, 7, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_6 (LeakyReLU)    (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 2, 2, 128)         262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_7 (LeakyReLU)    (None, 2, 2, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 512)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 527,489\n",
      "Trainable params: 527,489\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "height=18\n",
    "width=18\n",
    "channels=2\n",
    "\n",
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "x = layers.Conv2D(128, 3)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Use strided convolutions instead of max pooling as it lowers the sparsity of gradients\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(128, 4, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Dropout is essential to induce robustness to the GAN\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "discriminator = keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()\n",
    "\n",
    "#discriminator_optimizer = keras.optimizers.Adam(lr=0.0002, beta_1=0.5, clipvalue=1.0, decay=1e-8)\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
    "\n",
    "\n",
    "discriminator.compile(optimizer=discriminator_optimizer,\n",
    "                      loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN\n",
    "The gan itself is composed by the generator and the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "x = generator(gan_input)\n",
    "x = discriminator(x)\n",
    "gan = keras.models.Model(gan_input, x)\n",
    "\n",
    "#gan_optimizer = keras.optimizers.Adam(lr=0.0002, beta_1=0.5, clipvalue=1.0, decay=1e-8)\n",
    "gan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
    "\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "Load the data and scale them to be between 0.0 an 1.0. Further use zero-padding to get a 20x20 shape for this example. We use the already augmented data here for providing a large training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of 'xy_data': (160, 18, 18, 2)\n"
     ]
    }
   ],
   "source": [
    "x_position = np.load('data/x_pos_LASTEN2.npy')\n",
    "y_position = np.load('data/y_pos_LASTEN2.npy')\n",
    "\n",
    "offset = 0.0\n",
    "\n",
    "# Non existing points mapping\n",
    "x_position = np.where(x_position<=0, -offset, x_position) + offset\n",
    "y_position = np.where(x_position<=0, -offset, y_position) + offset\n",
    "\n",
    "x_position = x_position / (image_width + offset)\n",
    "y_position = y_position / (image_height + offset)\n",
    "\n",
    "x_position = x_position[:,:,:,np.newaxis]\n",
    "y_position = y_position[:,:,:,np.newaxis]\n",
    "\n",
    "xy_data = np.concatenate((x_position, y_position), axis=3)\n",
    "print(\"Shape of 'xy_data': {}\".format(xy_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Training\n",
    "The training of the DCGAN (Deep Convolutional Generative Adversarial Network) is a dynamic process, where an equilibrium between capability of the generator to fake images and the capability of the discriminator to recognize faked images should be achieved. The procedure of training is iterative. The following steps are repeated until a sufficient equilibrium is achieved:\n",
    "\n",
    "1. We randomly draw points from the latent space assuming a Gaussian distribution.\n",
    "2. The sample points from 1. are used to generate images with the generator.\n",
    "3. Generated images (fake) are mixed with images from the trainin set (real).\n",
    "4. Only the discriminator is trained where fake images get the label \"fake\" and real images have the label \"real\". In that way the disciminator learns to judge the generator whether is provided image is fake or real.\n",
    "5. Again draw random points from the latent space.\n",
    "6. The points from 5. are labeled as \"real\" images (although they are not) the parameters of the discriminator are fixed and the whole GAN model is trained. In that way the generator learns to fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discriminator loss: 0.6889187693595886\n",
      "adversarial loss: 0.6902413368225098\n",
      "discriminator loss: 0.9040408134460449\n",
      "adversarial loss: 0.7028865814208984\n",
      "discriminator loss: 0.6996284127235413\n",
      "adversarial loss: 0.7340842485427856\n",
      "discriminator loss: 0.6550412774085999\n",
      "adversarial loss: 0.7046524882316589\n",
      "discriminator loss: 0.699481189250946\n",
      "adversarial loss: 0.9441846609115601\n",
      "discriminator loss: 0.6572238802909851\n",
      "adversarial loss: 1.2079353332519531\n",
      "discriminator loss: 0.6801042556762695\n",
      "adversarial loss: 0.7557305097579956\n",
      "discriminator loss: 0.6185587048530579\n",
      "adversarial loss: 0.7194863557815552\n",
      "discriminator loss: 0.6302005052566528\n",
      "adversarial loss: 0.7338566780090332\n",
      "discriminator loss: 0.6436302661895752\n",
      "adversarial loss: 0.6477651000022888\n",
      "discriminator loss: 0.6569239497184753\n",
      "adversarial loss: 0.6734179258346558\n",
      "discriminator loss: 0.6466419100761414\n",
      "adversarial loss: 0.8108369708061218\n",
      "discriminator loss: 0.6617980003356934\n",
      "adversarial loss: 1.057755708694458\n",
      "discriminator loss: 0.7266647219657898\n",
      "adversarial loss: 0.7591331601142883\n",
      "discriminator loss: 0.6630620956420898\n",
      "adversarial loss: 0.7993279099464417\n",
      "discriminator loss: 0.66661137342453\n",
      "adversarial loss: 0.8823927640914917\n",
      "discriminator loss: 0.6651280522346497\n",
      "adversarial loss: 0.9276385307312012\n",
      "discriminator loss: 0.713801383972168\n",
      "adversarial loss: 0.8225322961807251\n",
      "discriminator loss: 0.6749550104141235\n",
      "adversarial loss: 0.8546479344367981\n",
      "discriminator loss: 0.6907995939254761\n",
      "adversarial loss: 0.6784107685089111\n"
     ]
    }
   ],
   "source": [
    "iterations = 40\n",
    "batch_size = 40\n",
    "save_dir = 'weights/gan'\n",
    "\n",
    "np.random.shuffle(xy_data)\n",
    "\n",
    "start = 0\n",
    "for step in range(iterations):\n",
    "    # Get random 'fake' images\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim)) # sample with mean=0 and std=1.0\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "    \n",
    "    # Get 'real' images, merge them with 'fake' and create labels\n",
    "    stop = start + batch_size\n",
    "    real_images = xy_data[start:stop]\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)) - 0.1,\n",
    "                             np.zeros((batch_size, 1))])\n",
    "    \n",
    "    labels += 0.05 * np.random.random(labels.shape) # important to introduce some randomness\n",
    "    \n",
    "    # Train the discriminator on 'real' and 'fake' images\n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "\n",
    "    # Get random images from generator but treat them as 'real' now\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    misleading_targets = np.zeros((batch_size, 1))\n",
    "    \n",
    "    # Train the generator's weights\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    # Finalize loop\n",
    "    start += batch_size\n",
    "    if start > len(xy_data) - batch_size:\n",
    "        start = 0\n",
    "        np.random.shuffle(xy_data)\n",
    "        \n",
    "    if step % 2 == 0:\n",
    "        gan.save_weights('gan.h5')\n",
    "        print('discriminator loss:', d_loss)\n",
    "        print('adversarial loss:', a_loss)\n",
    "        \n",
    "        gen = generated_images[0,:,:,0][:,:,np.newaxis]\n",
    "        real = real_images[0,:,:,0][:,:,np.newaxis]\n",
    "        \n",
    "        img = image.array_to_img(gen * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'generated' + str(step) + '.png'))\n",
    "        \n",
    "        img = image.array_to_img(real * 255., scale=False)\n",
    "        img.save(os.path.join(save_dir, 'real' + str(step) + '.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(160, 18, 18, 2)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xy_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize resultsm\n",
    "A randomly drawn latent vector is used to generate a fake image by the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_latent_vectors = np.random.normal(size=(batch_size, latent_dim)) # sample with mean=0 and std=1.0\n",
    "    \n",
    "generated_images = generator.predict(random_latent_vectors)\n",
    "generated_images *= 255\n",
    "\n",
    "for i in range(1):\n",
    "    img = generated_images[i,:,:,0]\n",
    "    img = img[:,:,np.newaxis]\n",
    "    img = keras.preprocessing.image.array_to_img(img)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
