{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network for Augmentation of Laser-Based Laryngeal Imaging\n",
    "\n",
    "For the deep-learning-based algorithm to match features via a registration task, it is essential to apply intense data augmentation to the training data set. As displayed in the following figure the training data consists of images $m(x)$ that represent the spatial configuration of laser points projected onto the vocal fold surface.\n",
    "\n",
    "![alt text here](images/feature_matching_registration.png \"Logo Title Jupyter Notebook logo\")\n",
    "\n",
    "The foundation for the images $m(x)$ are the x-y-coordinates of each single laser point within the image, as $m(x)$ is generated by plotting the single laser points and then smoothing the image. To create intense augmentation we want to train a generative adversaraial network (GAN) to then generate images that are variations of the images of the training set and represent feasible configurations of laser points projected onto a vocal fold. The implementation of the GAN is inspired by [Keras](https://keras.io/examples/generative/dcgan_overriding_train_step/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements\n",
    "The notebook was developed on Keras using the Tensorflow 2.2.0 backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Configuration\n",
    "Check for GPU and allow memory growth such that limitations for training are reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.get_visible_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters\n",
    "The grid dimensions have to be know, as well as the image dimensions for scaling. The depth of the input and output layer is 2 here, as we will have one channel representing x-coordinates and a second channel representing y-coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 18\n",
    "width = 18\n",
    "channels = 2\n",
    "\n",
    "image_width = 728\n",
    "image_height = 728"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "The dimension of the latent space can be adapted to optimize the network. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 8\n",
    "filters = 256"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "The first part of a GAN is a generator network that takes random input vectors from the latent space and decodes the vector to generate a synthetic image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense (Dense)                (None, 6400)              64000     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 10, 10, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 10, 10, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 20, 20, 256)       1048832   \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 20, 20, 256)       0         \n",
      "_________________________________________________________________\n",
      "conv2d (Conv2D)              (None, 20, 20, 2)         12802     \n",
      "=================================================================\n",
      "Total params: 2,174,466\n",
      "Trainable params: 2,174,466\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "x_factor = (width+2) // 4\n",
    "y_factor = (height+2) // 4\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim+1,)),\n",
    "        # We want to generate 128 coefficients to reshape into a 7x7x128 map\n",
    "        layers.Dense(x_factor * y_factor * filters),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((x_factor, y_factor, filters)),\n",
    "        layers.Conv2DTranspose(filters, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(filters, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(2, (x_factor, y_factor), padding=\"same\", activation=\"tanh\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "The second part of the GAN is a discriminator network that takes an image as input and decides if the image comes from the training set or was synthetically created by the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 10, 10, 128)       3584      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 5, 5, 256)         295168    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 5, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 6400)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 6401      \n",
      "=================================================================\n",
      "Total params: 305,153\n",
      "Trainable params: 305,153\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(width+2, height+2, 3)),\n",
    "        layers.Conv2D(filters//2, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(filters, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Flatten(), #layers.GlobalMaxPooling2D(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN\n",
    "The gan itself is composed by the generator and the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_labels = tf.round(tf.random.normal(shape=(batch_size, 1)) * 7.0)\n",
    "        random_latent_vectors = tf.concat([random_latent_vectors, random_labels], axis=1)\n",
    "        labeled_images = tf.reshape(tf.repeat(random_labels, repeats=20*20), (batch_size, 20, 20, 1))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "        generated_images = tf.concat([generated_images, labeled_images], axis=3)\n",
    "        \n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        random_labels = tf.round(tf.random.normal(shape=(batch_size, 1)) * 7.0)\n",
    "        random_latent_vectors = tf.concat([random_latent_vectors, random_labels], axis=1)\n",
    "        labeled_images = tf.reshape(tf.repeat(random_labels, repeats=20*20), (batch_size, 20, 20, 1))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            generated_images = self.generator(random_latent_vectors)\n",
    "            generated_images = tf.concat([generated_images, labeled_images], axis=3)\n",
    "            predictions = self.discriminator(generated_images)\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "Load the data and scale them to be between 0.0 an 1.0. Further use zero-padding to get a 20x20 shape for this example. Padding is applied to get 20x20 images, which is divisable twice by 2 such that two upconvolutions can de carried out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of 'xy_data': (160, 20, 20, 3)\n"
     ]
    }
   ],
   "source": [
    "x_position = np.load('data/x_pos_LASTEN2.npy')\n",
    "y_position = np.load('data/y_pos_LASTEN2.npy')\n",
    "\n",
    "offset = 0.0\n",
    "\n",
    "# Non existing points mapping\n",
    "x_position = np.where(x_position<=0, -offset, x_position) + offset\n",
    "y_position = np.where(x_position<=0, -offset, y_position) + offset\n",
    "\n",
    "x_position = x_position / (image_width + offset)\n",
    "y_position = y_position / (image_height + offset)\n",
    "\n",
    "# Zero padding\n",
    "x_position = np.pad(x_position,[(0,0),(1,1),(1,1)],constant_values=0)\n",
    "y_position = np.pad(y_position,[(0,0),(1,1),(1,1)],constant_values=0)\n",
    "\n",
    "x_position = x_position[:,:,:,np.newaxis]\n",
    "y_position = y_position[:,:,:,np.newaxis]\n",
    "\n",
    "label = np.zeros(x_position.shape)\n",
    "for i in range(len(x_position)):\n",
    "    label[i] = i\n",
    "xy_data = np.concatenate((x_position, y_position, label), axis=3)\n",
    "\n",
    "#xy_data = np.concatenate((x_position, y_position), axis=3)\n",
    "print(\"Shape of 'xy_data': {}\".format(xy_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Training\n",
    "The training of the DCGAN (Deep Convolutional Generative Adversarial Network) is a dynamic process, where an equilibrium between capability of the generator to fake images and the capability of the discriminator to recognize faked images should be achieved. See the \"train_step\" function of gan. The procedure of training is iterative. The following steps are repeated until a sufficient equilibrium is achieved:\n",
    "\n",
    "1. We randomly draw points from the latent space assuming a Gaussian distribution.\n",
    "2. The sample points from 1. are used to generate images with the generator.\n",
    "3. Generated images (fake) are mixed with images from the trainin set (real).\n",
    "4. Only the discriminator is trained where fake images get the label \"fake\" and real images have the label \"real\". In that way the disciminator learns to judge the generator whether is provided image is fake or real.\n",
    "5. Again draw random points from the latent space.\n",
    "6. The points from 5. are labeled as \"real\" images (although they are not) the parameters of the discriminator are fixed and the whole GAN model is trained. In that way the generator learns to fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = keras.preprocessing.image.array_to_img(np.repeat(generated_images[i][:,:,0][:,:,np.newaxis],3,axis=2))\n",
    "            img.save(\"gan_2/generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.3918 - g_loss: 12.9650\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2166 - g_loss: 14.1975\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 1s 78ms/step - d_loss: 7.2688 - g_loss: 14.7522\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 1s 78ms/step - d_loss: 7.2288 - g_loss: 14.4246\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 1s 78ms/step - d_loss: 7.2644 - g_loss: 14.6637\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2368 - g_loss: 14.4983\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2460 - g_loss: 14.0902\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2306 - g_loss: 14.0177\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2373 - g_loss: 14.5196\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2337 - g_loss: 14.1209\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2125 - g_loss: 14.1200\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2324 - g_loss: 13.8237\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2279 - g_loss: 14.3998\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2330 - g_loss: 15.1206\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2603 - g_loss: 15.2492\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2601 - g_loss: 15.2492\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2436 - g_loss: 15.2492\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2381 - g_loss: 15.2492\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2546 - g_loss: 15.2492\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2343 - g_loss: 15.2492\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2510 - g_loss: 15.2492\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2365 - g_loss: 15.2492\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2184 - g_loss: 15.2492\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2577 - g_loss: 15.2492\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2659 - g_loss: 15.2492\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2610 - g_loss: 15.2492\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2567 - g_loss: 15.2492\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2295 - g_loss: 15.2492\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2277 - g_loss: 15.2492\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 1s 79ms/step - d_loss: 7.2205 - g_loss: 15.2492\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f3d2c0e1700>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 30\n",
    "batch_size = 20\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.0003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy()#from_logits=True),\n",
    ")\n",
    "\n",
    "gan.fit(xy_data, batch_size=batch_size, epochs=epochs)#, callbacks=[GANMonitor(num_img=3, latent_dim=latent_dim)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize results\n",
    "A randomly drawn latent vector is used to generate a fake image by the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_latent_vectors = np.random.normal(size=(batch_size, latent_dim)) # sample with mean=0 and std=1.0\n",
    "random_labels = np.round(np.random.normal(size=(batch_size, 1)) * 7.0)     \n",
    "\n",
    "random_latent_vectors = np.concatenate((random_latent_vectors, random_labels), axis =1)\n",
    "    \n",
    "generated_images = generator.predict(random_latent_vectors)\n",
    "generated_images *= 255\n",
    "\n",
    "for i in range(1):\n",
    "    \n",
    "    img = generated_images[i,:,:,0]\n",
    "    img = img[:,:,np.newaxis]\n",
    "    img = keras.preprocessing.image.array_to_img(img)\n",
    "    plt.imshow(img)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
