{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "try:\n",
    "  # Disable all GPUS\n",
    "  tf.config.set_visible_devices([], 'GPU')\n",
    "  visible_devices = tf.config.get_visible_devices()\n",
    "  for device in visible_devices:\n",
    "    assert device.device_type != 'GPU'\n",
    "except:\n",
    "  # Invalid device or cannot modify virtual devices once initialized.\n",
    "  pass\n",
    "\n",
    "print(tf.config.get_visible_devices('GPU'))\n",
    "print(tf.config.get_visible_devices('CPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.2.0-rc3\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(tf.__version__)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.get_visible_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_width = 18\n",
    "grid_height = 18\n",
    "inout_layers = 2\n",
    "\n",
    "image_width = 728\n",
    "image_height = 728\n",
    "\n",
    "batch_size=20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_path = '/home/julian/Documents/Studium/MT-Masterarbeit/Data/LASTEN2/train'\n",
    "\n",
    "files = glob.glob(base_path+'/*.json')\n",
    "\n",
    "file_length =len(files)\n",
    "\n",
    "x_position = np.full((file_length, grid_height, grid_width), 0)\n",
    "y_position = np.full((file_length, grid_height, grid_width), 0)\n",
    "\n",
    "for file_id, file in enumerate(files):\n",
    "    with open(file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        \n",
    "        for key, value in data.items():\n",
    "            key = int(key)\n",
    "        \n",
    "            \n",
    "            y = key // grid_height \n",
    "            x = key % grid_width\n",
    "\n",
    "            x_position[file_id][y][x] = value[0]\n",
    "            y_position[file_id][y][x] = value[1]\n",
    "            \n",
    "np.save('x_pos', x_position)\n",
    "np.save('y_pos', y_position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_position = np.load('x_pos.npy')\n",
    "y_position = np.load('y_pos.npy')\n",
    "\n",
    "x_position = x_position / image_width\n",
    "y_position = y_position / image_height\n",
    "\n",
    "a = x_position\n",
    "\n",
    "# Zero padding\n",
    "x_position = np.pad(x_position,[(0,0),(1,1),(1,1)],constant_values=0)\n",
    "y_position = np.pad(y_position,[(0,0),(1,1),(1,1)],constant_values=0)\n",
    "\n",
    "x_position = x_position[:,:,:,np.newaxis]\n",
    "y_position = y_position[:,:,:,np.newaxis]\n",
    "\n",
    "xy_data = np.concatenate((x_position, y_position), axis=3)\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(xy_data)\n",
    "dataset = dataset.shuffle(buffer_size=1024).batch(batch_size).prefetch(32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reconstruct_image(x_position, y_position):\n",
    "    # Unpad\n",
    "    x_position = x_position[1:-1, 1:-1]\n",
    "    y_position = y_position[1:-1, 1:-1]\n",
    "    \n",
    "    image = np.zeros((image_height, image_width))\n",
    "    \n",
    "    for x, y in zip(x_position.flatten(), y_position.flatten()):\n",
    "        \n",
    "        x = int(x*image_width)\n",
    "        y = int(y*image_height)\n",
    "        \n",
    "        if x >= 20 and y>= 20:\n",
    "            image[y][x] = 1\n",
    "        \n",
    "    image = endolas.utils.apply_smoothing(image, sigma=2, sigma_back=15)\n",
    "    \n",
    "    return image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 10, 10, 64)        2112      \n",
      "_________________________________________________________________\n",
      "leaky_re_lu (LeakyReLU)      (None, 10, 10, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 5, 5, 128)         131200    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)    (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "global_max_pooling2d (Global (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 133,441\n",
      "Trainable params: 133,441\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "discriminator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(grid_height+2, grid_width+2, 2)),\n",
    "        layers.Conv2D(64, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.GlobalMaxPooling2D(),\n",
    "        #layers.Flatten(),\n",
    "        #layers.Dropout(0.4),\n",
    "        layers.Dense(1),\n",
    "    ],\n",
    "    name=\"discriminator\",\n",
    ")\n",
    "\n",
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 3200)              22400     \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)    (None, 3200)              0         \n",
      "_________________________________________________________________\n",
      "reshape (Reshape)            (None, 5, 5, 128)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose (Conv2DTran (None, 10, 10, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)    (None, 10, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_transpose_1 (Conv2DTr (None, 20, 20, 128)       262272    \n",
      "_________________________________________________________________\n",
      "leaky_re_lu_4 (LeakyReLU)    (None, 20, 20, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 20, 20, 2)         12546     \n",
      "=================================================================\n",
      "Total params: 559,490\n",
      "Trainable params: 559,490\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 6\n",
    "\n",
    "x_factor = (grid_height+2) // 4\n",
    "y_factor = (grid_width+2) // 4\n",
    "\n",
    "generator = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=(latent_dim,)),\n",
    "        # We want to generate 128 coefficients to reshape into a 7x7x128 map\n",
    "        layers.Dense(y_factor * x_factor * 128),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Reshape((y_factor, x_factor, 128)),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2DTranspose(128, (4, 4), strides=(2, 2), padding=\"same\"),\n",
    "        layers.LeakyReLU(alpha=0.2),\n",
    "        layers.Conv2D(2, (7, 7), padding=\"same\", activation=\"sigmoid\"),\n",
    "    ],\n",
    "    name=\"generator\",\n",
    ")\n",
    "\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# GAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(keras.Model):\n",
    "    def __init__(self, discriminator, generator, latent_dim):\n",
    "        super(GAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, loss_fn):\n",
    "        super(GAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "        # Sample random points in the latent space\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Decode them to fake images\n",
    "        generated_images = self.generator(random_latent_vectors)\n",
    "\n",
    "        # Combine them with real images\n",
    "        combined_images = tf.concat([generated_images, real_images], axis=0)\n",
    "\n",
    "        # Assemble labels discriminating real from fake images\n",
    "        labels = tf.concat(\n",
    "            [tf.ones((batch_size, 1)), tf.zeros((batch_size, 1))], axis=0\n",
    "        )\n",
    "        # Add random noise to the labels - important trick!\n",
    "        labels += 0.05 * tf.random.uniform(tf.shape(labels))\n",
    "\n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(combined_images)\n",
    "            d_loss = self.loss_fn(labels, predictions)\n",
    "        grads = tape.gradient(d_loss, self.discriminator.trainable_weights)\n",
    "        self.d_optimizer.apply_gradients(\n",
    "            zip(grads, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # Sample random points in the latent space\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "\n",
    "        # Assemble labels that say \"all real images\"\n",
    "        misleading_labels = tf.zeros((batch_size, 1))\n",
    "\n",
    "        # Train the generator (note that we should *not* update the weights\n",
    "        # of the discriminator)!\n",
    "        with tf.GradientTape() as tape:\n",
    "            predictions = self.discriminator(self.generator(random_latent_vectors))\n",
    "            g_loss = self.loss_fn(misleading_labels, predictions)\n",
    "        grads = tape.gradient(g_loss, self.generator.trainable_weights)\n",
    "        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=3, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = generated_images[i,:,:,0]\n",
    "            img = img[:,:,np.newaxis]\n",
    "            img = keras.preprocessing.image.array_to_img(img)\n",
    "            img.save(\"gan_lasten/generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "8/8 [==============================] - 0s 37ms/step - d_loss: 0.6984 - g_loss: 0.6715\n",
      "Epoch 2/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.6917 - g_loss: 0.6677\n",
      "Epoch 3/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.6542 - g_loss: 0.7183\n",
      "Epoch 4/30\n",
      "8/8 [==============================] - 0s 27ms/step - d_loss: 0.6141 - g_loss: 0.7803\n",
      "Epoch 5/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.5779 - g_loss: 0.8359\n",
      "Epoch 6/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.6128 - g_loss: 0.7391\n",
      "Epoch 7/30\n",
      "8/8 [==============================] - 0s 29ms/step - d_loss: 0.7047 - g_loss: 0.5844\n",
      "Epoch 8/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.6995 - g_loss: 0.6151\n",
      "Epoch 9/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.6603 - g_loss: 0.7062\n",
      "Epoch 10/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.6186 - g_loss: 0.8085\n",
      "Epoch 11/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.5841 - g_loss: 0.8934\n",
      "Epoch 12/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.5715 - g_loss: 0.9157\n",
      "Epoch 13/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.5763 - g_loss: 0.8955\n",
      "Epoch 14/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.5793 - g_loss: 0.8585\n",
      "Epoch 15/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.5955 - g_loss: 0.8047\n",
      "Epoch 16/30\n",
      "8/8 [==============================] - 0s 30ms/step - d_loss: 0.6166 - g_loss: 0.7416\n",
      "Epoch 17/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.6017 - g_loss: 0.7766\n",
      "Epoch 18/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.5880 - g_loss: 0.8006\n",
      "Epoch 19/30\n",
      "8/8 [==============================] - 0s 29ms/step - d_loss: 0.5870 - g_loss: 0.8006\n",
      "Epoch 20/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.5745 - g_loss: 0.8138\n",
      "Epoch 21/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.5520 - g_loss: 0.8467\n",
      "Epoch 22/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.5392 - g_loss: 0.8780\n",
      "Epoch 23/30\n",
      "8/8 [==============================] - 0s 29ms/step - d_loss: 0.5431 - g_loss: 0.8551\n",
      "Epoch 24/30\n",
      "8/8 [==============================] - 0s 29ms/step - d_loss: 0.5304 - g_loss: 0.8739\n",
      "Epoch 25/30\n",
      "8/8 [==============================] - 0s 29ms/step - d_loss: 0.5313 - g_loss: 0.8720\n",
      "Epoch 26/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.6284 - g_loss: 0.6869\n",
      "Epoch 27/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.6942 - g_loss: 0.5935\n",
      "Epoch 28/30\n",
      "8/8 [==============================] - 0s 28ms/step - d_loss: 0.6648 - g_loss: 0.7108\n",
      "Epoch 29/30\n",
      "8/8 [==============================] - 0s 29ms/step - d_loss: 0.6388 - g_loss: 0.8632\n",
      "Epoch 30/30\n",
      "8/8 [==============================] - 0s 29ms/step - d_loss: 0.6264 - g_loss: 0.9694\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x7f4e24243ac0>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 30\n",
    "\n",
    "gan = GAN(discriminator=discriminator, generator=generator, latent_dim=latent_dim)\n",
    "gan.compile(\n",
    "    d_optimizer=keras.optimizers.Adam(learning_rate=0.00003),\n",
    "    g_optimizer=keras.optimizers.Adam(learning_rate=0.003),\n",
    "    loss_fn=keras.losses.BinaryCrossentropy(from_logits=True),\n",
    ")\n",
    "\n",
    "gan.fit(\n",
    "    xy_data, batch_size=batch_size, epochs=epochs, callbacks=[GANMonitor(num_img=1, latent_dim=latent_dim)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor([[ 0.3175086  -0.04547724 -0.07965992  0.56269747  0.53410584  1.2879528 ]], shape=(1, 6), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "random_latent_vectors = tf.random.normal(shape=(1, latent_dim))\n",
    "print(random_latent_vectors)\n",
    "arg = tf.convert_to_tensor(np.array([[0,0,0,0,0,0]]), dtype=tf.float32)\n",
    "generated_images = generator(random_latent_vectors)\n",
    "generated_images *= 255\n",
    "generated_images.numpy()\n",
    "for i in range(1):\n",
    "    img = generated_images[i,:,:,0]\n",
    "    img = img[:,:,np.newaxis]\n",
    "    img = keras.preprocessing.image.array_to_img(img)\n",
    "    plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
