{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generative Adversarial Network for Augmentation of Laser-Based Laryngeal Imaging\n",
    "\n",
    "For the deep-learning-based algorithm to match features via a registration task, it is essential to apply intense data augmentation to the training data set. The training data consists of images $m(x)$ that represent the spatial configuration of laser points projected onto the vocal fold surface.\n",
    "The foundation for the images $m(x)$ are the x-y-coordinates of each single laser point within the image, as $m(x)$ is generated by plotting the single laser points and then smoothing the image. To create intense augmentation we want to train a generative adversaraial network (GAN) to generate images that are variations of the images of the training set and represent feasible configurations of laser points projected onto a vocal fold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Statements\n",
    "The notebook was developed on Keras using the Tensorflow 2.2.0 backend."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras as keras\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.ndimage import gaussian_filter\n",
    "import glob\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hardware Configuration\n",
    "Check for GPU and allow memory growth such that limitations for training are reduced. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tf.__version__)\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "tf.config.experimental.get_visible_devices('GPU')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model parameters\n",
    "The grid dimensions have to be know, as well as the image dimensions for scaling. The depth of the input and output layer is 2 here, as we will have one channel representing x-coordinates and a second channel representing y-coordinates. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height = 18\n",
    "width = 18\n",
    "channels = 2\n",
    "\n",
    "image_width = 728\n",
    "image_height = 728"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters\n",
    "The dimension of the latent space can be adapted to optimize the network. Further, kernel size for convolutional layers and filters can be set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 8\n",
    "kernel_size = 4\n",
    "filters = 128"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generator\n",
    "The first part of a GAN is a generator network that takes random input vectors from the latent space and decodes the vector to generate a synthetic image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator_input = keras.Input(shape=(latent_dim,))\n",
    "\n",
    "# Leaky ReLU is preferred as it lowers the sparsity of gradients\n",
    "x = layers.Dense(filters * (height//2) * (width//2))(generator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Reshape(((height//2), (width//2), filters))(x)\n",
    "\n",
    "# Kernel size should be divisible by the stride size to prevent checkerboard artifacts\n",
    "x = layers.Conv2DTranspose(2*filters, kernel_size, strides=2, padding='same')(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Use tanh activation for improved training\n",
    "x = layers.Conv2D(channels, kernel_size, activation='tanh', padding='same')(x)\n",
    "generator = keras.models.Model(generator_input, x)\n",
    "generator.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discriminator\n",
    "The second part of the GAN is a discriminator network that takes an image as input and decides if the image comes from the training set or was synthetically created by the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height=18\n",
    "width=18\n",
    "channels=2\n",
    "\n",
    "discriminator_input = layers.Input(shape=(height, width, channels))\n",
    "\n",
    "x = layers.Conv2D(filters, kernel_size)(discriminator_input)\n",
    "x = layers.LeakyReLU()(x)\n",
    "\n",
    "# Use strided convolutions instead of max pooling as it lowers the sparsity of gradients\n",
    "x = layers.Conv2D(filters, kernel_size, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Conv2D(filters, kernel_size, strides=2)(x)\n",
    "x = layers.LeakyReLU()(x)\n",
    "x = layers.Flatten()(x)\n",
    "\n",
    "# Dropout is essential to induce robustness to the GAN\n",
    "x = layers.Dropout(0.5)(x)\n",
    "\n",
    "x = layers.Dense(1, activation='sigmoid')(x)\n",
    "\n",
    "discriminator = keras.models.Model(discriminator_input, x)\n",
    "discriminator.summary()\n",
    "\n",
    "#discriminator_optimizer = keras.optimizers.Adam(lr=0.0002, beta_1=0.5, clipvalue=1.0, decay=1e-8)\n",
    "discriminator_optimizer = keras.optimizers.RMSprop(lr=0.0002, clipvalue=1.0, decay=1e-8)\n",
    "\n",
    "discriminator.compile(optimizer=discriminator_optimizer,\n",
    "                      loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN\n",
    "The gan itself is composed by the generator and the discriminator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator.trainable = False\n",
    "\n",
    "gan_input = keras.Input(shape=(latent_dim,))\n",
    "x = generator(gan_input)\n",
    "x = discriminator(x)\n",
    "gan = keras.models.Model(gan_input, x)\n",
    "\n",
    "#gan_optimizer = keras.optimizers.Adam(lr=0.0002, beta_1=0.5, clipvalue=1.0, decay=1e-8)\n",
    "gan_optimizer = keras.optimizers.RMSprop(lr=0.0004, clipvalue=1.0, decay=1e-8)\n",
    "\n",
    "gan.compile(optimizer=gan_optimizer, loss='binary_crossentropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prepare data\n",
    "Load the data and scale it to be between 0.0 an 1.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a path from where to load the data\n",
    "base_path = 'Data/LASTEN/train'\n",
    "\n",
    "# Load data into numpy arrays\n",
    "def sort_key(path):\n",
    "    return int(path.split(os.sep)[-1].split(\".\")[0].split(\"_\")[0]) \n",
    "    \n",
    "globs = glob.glob(base_path+'/*.json')\n",
    "files = sorted(globs, key=sort_key)\n",
    "file_length =len(files)\n",
    "x_position = np.full((file_length, height, width), 0)\n",
    "y_position = np.full((file_length, height, width), 0)\n",
    "for file_id, file in enumerate(files):\n",
    "    with open(file) as json_file:\n",
    "        data = json.load(json_file)\n",
    "        \n",
    "        for key, value in data.items():\n",
    "            key = int(key)\n",
    "        \n",
    "            \n",
    "            y = key // height \n",
    "            x = key % width\n",
    "\n",
    "            x_position[file_id][y][x] = value[0]\n",
    "            y_position[file_id][y][x] = value[1]\n",
    "\n",
    "# Define offset\n",
    "offset = 0.0\n",
    "\n",
    "# Non existing points mapping\n",
    "x_position = np.where(x_position<=0, -offset, x_position) + offset\n",
    "y_position = np.where(x_position<=0, -offset, y_position) + offset\n",
    "\n",
    "x_position = x_position / (image_width + offset)\n",
    "y_position = y_position / (image_height + offset)\n",
    "\n",
    "x_position = x_position[:,:,:,np.newaxis]\n",
    "y_position = y_position[:,:,:,np.newaxis]\n",
    "\n",
    "#x_position = x_position[0:20, :, : ,:]\n",
    "#y_position = y_position[0:20, :, : ,:]\n",
    "\n",
    "xy_data = np.concatenate((x_position, y_position), axis=3)\n",
    "print(\"Shape of 'xy_data': {}\".format(xy_data.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GAN Training\n",
    "The training of the DCGAN (Deep Convolutional Generative Adversarial Network) is a dynamic process, where an equilibrium between the capability of the generator to fake images and the capability of the discriminator to recognize faked images should be achieved. The procedure of training is iterative. The following steps are repeated until a sufficient equilibrium is achieved:\n",
    "\n",
    "1. We randomly draw points from the latent space assuming a Gaussian distribution.\n",
    "2. The sample points from 1. are used to generate images with the generator.\n",
    "3. Generated images (fake) are mixed with images from the training set (real).\n",
    "4. Only the discriminator is trained where fake images get the label \"fake\" and real images have the label \"real\". In that way the disciminator learns to judge the generator whether the provided image is fake or real.\n",
    "5. Again draw random points from the latent space.\n",
    "6. The points from 5. are labeled as \"real\" images (although they are not) the parameters of the discriminator are fixed and the whole GAN model is trained. In that way the generator learns to fake images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "iterations = 40\n",
    "batch_size = 5\n",
    "save_dir = 'weights/gan'\n",
    "\n",
    "xy_data_orig = xy_data.copy()\n",
    "np.random.shuffle(xy_data)\n",
    "\n",
    "start = 0\n",
    "for step in range(iterations):\n",
    "    # Get random 'fake' images\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim)) # sample with mean=0 and std=1.0\n",
    "    generated_images = generator.predict(random_latent_vectors)\n",
    "    \n",
    "    # Get 'real' images, merge them with 'fake' and create labels\n",
    "    stop = start + batch_size\n",
    "    real_images = xy_data[start:stop]\n",
    "    combined_images = np.concatenate([generated_images, real_images])\n",
    "    labels = np.concatenate([np.ones((batch_size, 1)) - 0.1,\n",
    "                             np.zeros((batch_size, 1))])\n",
    "    \n",
    "    labels += 0.05 * np.random.random(labels.shape) # important to introduce some randomness\n",
    "    \n",
    "    # Train the discriminator on 'real' and 'fake' images\n",
    "    d_loss = discriminator.train_on_batch(combined_images, labels)\n",
    "\n",
    "    # Get random images from generator but treat them as 'real' now\n",
    "    random_latent_vectors = np.random.normal(size=(batch_size, latent_dim))\n",
    "    misleading_targets = np.zeros((batch_size, 1))\n",
    "    \n",
    "    # Train the generator's weights\n",
    "    a_loss = gan.train_on_batch(random_latent_vectors, misleading_targets)\n",
    "    \n",
    "    # Finalize loop\n",
    "    start += batch_size\n",
    "    if start > len(xy_data) - batch_size:\n",
    "        start = 0\n",
    "        np.random.shuffle(xy_data)\n",
    "        \n",
    "    # Print loss\n",
    "    print('discriminator loss:', d_loss)\n",
    "    print('adversarial loss:', a_loss)      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict images and compare with ground truth\n",
    "Following cells will display results of the prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Predicition of x-coordinates by generator\n",
    "A randomly drawn latent vector is used to generate fake images by the generator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random_latent_vectors = np.random.normal(size=(8, latent_dim))# sample with mean=0 and std=1.0\n",
    "generated_images = generator.predict(random_latent_vectors)\n",
    "\n",
    "plt.rcParams[\"figure.figsize\"] = (7,7)\n",
    "for i in range(8):\n",
    "    img = generated_images[i,:,:,1]\n",
    "    img = img[:,:,np.newaxis]\n",
    "    img = keras.preprocessing.image.array_to_img(img)\n",
    "    \n",
    "    val = 441 + i\n",
    "    plt.subplot(val)\n",
    "    plt.imshow(img)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ground-truth of x-coordinates\n",
    "Out of all 8 recordings from the dataset one frame is displayed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "for i in range(8):\n",
    "    val = 441 + i\n",
    "    plt.subplot(val)\n",
    "    x = xy_data_orig[i*20,:,:,0]\n",
    "    plt.imshow(x)\n",
    "    plt.axis('off')\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conclusion: Predictions of the GAN are not even close to the ground truth! Sampling data from the GAN is therefore not reasonable."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
